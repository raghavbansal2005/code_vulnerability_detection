import argparse
from util import str2bool, load_config
from pathlib import Path
import os
import wandb
import torch
from torch import _pin_memory, logit, nn
import pytorch_lightning as pl
from pytorch_lightning.loggers import WandbLogger
from pytorch_lightning.strategies import DDPStrategy
import torch
import torchmetrics
from torchmetrics import Accuracy, Recall, Precision
import numpy as np
from data.dataset import CodeFile
from transformers import get_linear_schedule_with_warmup
from transformers import RobertaTokenizer
from tokenizers import Tokenizer
# from models.bert import BERTforPreTraining
from models.rankfromsets import InnerProduct
from models.lstm import LSTM_model
from typing import Optional
import warnings
from dotenv import load_dotenv
import pickle

warnings.filterwarnings("ignore")


class PreTrainingModule(pl.LightningModule):
    def __init__(self, model, run_d):
        super().__init__()
        self.model = model
        self.run_d = run_d
        self.cross_loss = nn.BCEWithLogitsLoss()
        self.learning_rate = self.run_d["learning_rate"]
        self.accuracy = torchmetrics.Accuracy()
        self.precision_metric = torchmetrics.Precision()
        self.recall_metric = torchmetrics.Recall()
        self.save_hyperparameters()
        self.val_predictions = []
        self.val_labels = []
        self.counter_variable = 1

    def training_step(self, batch, batch_idx):
        metrics_labels = batch.pop("metrics_labels")
        logits = self.model(**batch)
        loss = self.cross_loss(logits, batch["labels"].float())
        self.log("train_loss", loss)
        return loss
    
    def validation_step(self, batch, batch_idx):
        metrics_labels = batch.pop("metrics_labels")
        logits = self.model(**batch)
        print()
        preds = torch.argmax(logits, dim=1)
        self.accuracy.update(preds, metrics_labels)
        self.precision_metric.update(preds, metrics_labels)
        self.recall_metric.update(preds, metrics_labels)
        preds = np.argmax(logits.detach().cpu().numpy(), axis=1)
        self.val_predictions.extend(preds)
        
    def test_step(self, batch, batch_idx):
        metrics_labels = batch.pop("metrics_labels")
        logits = self.model(**batch)
        preds = torch.argmax(logits, dim=1)
        self.accuracy.update(preds, metrics_labels)
        self.precision_metric.update(preds, metrics_labels)
        self.recall_metric.update(preds, metrics_labels)
        preds = np.argmax(logits.detach().cpu().numpy(), axis=1)
        self.val_predictions.extend(preds)

    def configure_optimizers(self):
        # set up optimizer
        if self.run_d["optimizer"] == "adam":
            optimizer = torch.optim.Adam(
                self.model.parameters(),
                lr=self.run_d["learning_rate"],
                betas=eval(self.run_d["betas"]),
                weight_decay=self.run_d["weight_decay"],
            )
        elif self.run_d["optimizer"] == "adamW":
            optimizer = torch.optim.AdamW(
                self.model.parameters(), lr=self.run_d["learning_rate"], weight_decay=self.run_d["weight_decay"]
            )
        else:
            raise NotImplementedError

        # set up scheduler
        if self.run_d["scheduler"] == "step":
            scheduler = torch.optim.lr_scheduler.StepLR(
                optimizer, self.run_d["scheduler_period"], gamma=self.run_d["scheduler_ratio"]
            )
        elif self.run_d["scheduler"] == "plateau":
            scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
                optimizer, mode="min", patience=self.run_d["scheduler_period"], factor=self.run_d["scheduler_ratio"]
            )
        elif self.run_d["scheduler"] == "linear_with_warmup":
            scheduler = get_linear_schedule_with_warmup(
                optimizer, num_warmup_steps=self.num_training_steps * 0.1, num_training_steps=self.num_training_steps
            )
        else:
            return optimizer

        return [optimizer], [{"scheduler": scheduler, "interval": "step"}]

    def training_epoch_end(self, outputs):
        if self.global_step >= 0.9 * self.num_training_steps:
            self.model.max_len = 512

        if self.trainer.is_global_zero and self.run_d["save_state"]:
            self.save_model()

    def validation_epoch_end(self, outputs):
        with open(f'val_preds_{self.counter_variable}.pkl', 'wb') as f:
            pickle.dump(self.val_predictions, f)

        self.val_predictions = []
        self.counter_variable += 1
        self.log("Validation Accuracy", self.accuracy.compute())
        self.accuracy.reset()
        self.log("Validation Precision", self.precision_metric.compute())
        self.precision_metric.reset()
        self.log("Validation Recall", self.recall_metric.compute())
        self.recall_metric.reset()

    def test_epoch_end(self, outputs):
        self.log("Test Accuracy", self.accuracy.compute())
        self.accuracy.reset()
        self.log("Test Precision", self.precision_metric.compute())
        self.precision_metric.reset()
        self.log("Test Recall", self.recall_metric.compute())
        self.recall_metric.reset()

    def save_model(self):
        mr_fp = os.path.join(wandb.run.dir, "bert-pre-train.pt")
        torch.save(self.model.state_dict(), mr_fp)

    @property
    def num_training_steps(self) -> int:
        """Total training steps inferred from datamodule and devices."""
        if self.trainer.max_steps:
            return self.trainer.max_steps

        limit_batches = self.trainer.limit_train_batches
        batches = len(self.train_dataloader())
        batches = min(batches, limit_batches) if isinstance(limit_batches, int) else int(limit_batches * batches)

        num_devices = max(1, self.trainer.num_gpus, self.trainer.num_processes)
        if self.trainer.tpu_cores:
            num_devices = max(num_devices, self.trainer.tpu_cores)

        effective_accum = self.trainer.accumulate_grad_batches * num_devices
        return (batches // effective_accum) * self.trainer.max_epochs


class TrainingDataModule(pl.LightningDataModule):
    def __init__(self, data_d, run_d, collate):
        super().__init__()
        self.data_d = data_d
        self.run_d = run_d
        self.collate = collate

    def setup(self, stage: Optional[str] = None) -> None:
        self.train_data = CodeFile(self.data_d["train_path"])
        self.val_data = CodeFile(self.data_d["val_path"])
        self.test_data = CodeFile(self.data_d["test_path"])

    def train_dataloader(self) -> torch.utils.data.DataLoader:
        return torch.utils.data.DataLoader(
            self.train_data,
            batch_size=self.run_d["batch_size"],
            shuffle=True,
            collate_fn=self.collate,
            pin_memory=True,
            num_workers=self.run_d["num_workers"],
        )
    
    def val_dataloader(self) -> torch.utils.data.DataLoader:
        return torch.utils.data.DataLoader(
            self.val_data,
            batch_size = self.run_d["eval_batch_size"],
            shuffle=False,
            collate_fn=self.collate,
            pin_memory=True,
            num_workers=self.run_d["num_workers"]
        )

    def test_dataloader(self) -> torch.utils.data.DataLoader:
        return torch.utils.data.DataLoader(
            self.test_data,
            batch_size = self.run_d["batch_size"],
            shuffle=False,
            collate_fn=self.collate,
            pin_memory=True,
            num_workers=self.run_d["num_workers"]
        )

if __name__ == "__main__":

    load_dotenv()

    parser = argparse.ArgumentParser(description="Train")
    parser.add_argument(
        "-t", "--template_fp", type=str, default="config/template.yaml", help="path to template config file"
    )
    parser.add_argument("-c", "--custom_fp", type=str, required=False, help="path to custom config file")
    parser.add_argument("-l", "--log_dir", type=str, default="./", help="path to log/output directory")
    flags = parser.parse_args()

    project_name, run_name, data_d, model_d, run_d = load_config(flags.template_fp, flags.custom_fp)
    Path(flags.log_dir).mkdir(parents=True, exist_ok=True)

    wandb_logger = WandbLogger(project=project_name, name=run_name, save_dir=flags.log_dir)
    trainer = pl.Trainer(
        max_epochs=run_d["num_epochs"],
        gpus=torch.cuda.device_count(),
        logger=wandb_logger,
        strategy=DDPStrategy(find_unused_parameters=False),
        precision=16,
        profiler="simple",
        default_root_dir=flags.log_dir,
    )

    tokenizer = RobertaTokenizer.from_pretrained("roberta-base")
    if model_d["model_name"] == "rankfromsets":
        model_dict = {"n_labels":model_d["classes"],"n_attributes":len(tokenizer),"emb_size":model_d["emb_size"],"sparse":False,"mode":model_d["mode"]}
        model = InnerProduct(**model_dict)
    elif model_d["model_name"] == "lstm":
        model_dict = {"num_words":len(tokenizer),"emb_size":model_d["emb_size"],"outputs":model_d["n_labels"],"dropout":model_d["dropout"]}
        model = LSTM_model(**model_dict)
    
    

    data_module = TrainingDataModule(data_d, run_d, model.collate_fn_binary)
    if "load_state" in run_d and run_d["load_state"]:
        pre_training_module = PreTrainingModule.load_from_checkpoint(run_d["chkpt_path"])
    else:
        pre_training_module = PreTrainingModule(model, run_d)

    trainer.fit(pre_training_module, data_module)
    trainer.test(pre_training_module, data_module)