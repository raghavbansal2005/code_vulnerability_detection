import torch
import numpy as np


import torch
import torch.nn as nn
from torch.nn.utils.rnn import pack_padded_sequence


class LSTM_model(nn.Module):
    def __init__(self, num_words, emb_size, outputs, dropout):
        super().__init__()
        self.word_embeddings = nn.Embedding(num_words, emb_size)
        self.lstm = nn.LSTM(emb_size, emb_size, dropout=dropout, batch_first = True)
        self.linear = nn.Linear(emb_size, outputs)
        self.reset_parameters()

    def reset_parameters(self):
        for module in [self.word_embeddings, self.linear]:
            scale = 0.07
            nn.init.uniform_(module.weight, -scale, scale)

    def collate_fn(self, examples, **kwargs):
        words = []
        num_words = [len(x['tokenized_code']) for x in examples]
        for example in examples:
            word = example['tokenized_code']
            word.extend([0] * (max(num_words) - len(word)))
            words.append(word)
            
        labels = torch.tensor(np.stack([x["vulnerability_id"] for x in examples], axis = 0), dtype=torch.long)

        words = torch.tensor(words, dtype=torch.long)
        lengths = torch.tensor(num_words, dtype=torch.int64)

        labels_arr = torch.zeros((len(examples), 256), dtype=torch.long)
        for i in range(len(labels)):
            labels_arr[i][labels[i]] = 1

        return {"x":words, "lengths":lengths, "labels" : labels_arr, "metrics_labels": labels.squeeze()}

    def collate_fn_binary(self, examples, **kwargs):
        words = []
        num_words = [len(x['tokenized_code']) for x in examples]
        for example in examples:
            word = example['tokenized_code']
            word.extend([0] * (max(num_words) - len(word)))
            words.append(word)
            
        labels = torch.tensor(np.stack([x["binary_label"] for x in examples], axis = 0), dtype=torch.long)

        words = torch.tensor(words, dtype=torch.long)
        lengths = torch.tensor(num_words, dtype=torch.int64)

        labels_arr = torch.zeros((len(examples), 2), dtype=torch.long)
        for i in range(len(labels)):
            labels_arr[i][labels[i]] = 1


        return {"x":words, "lengths":lengths, "labels" : labels_arr, "metrics_labels": labels.squeeze()}

    def forward(self, x, lengths, **kwargs):

        x = self.word_embeddings(x)
        x_pack = pack_padded_sequence(x, lengths.cpu(), batch_first=True, enforce_sorted=False)
        outputs, (hn, cn) = self.lstm(x_pack)

        hidden = hn[-1]
        
        dense_outputs=self.linear(hidden)

        return dense_outputs.squeeze()