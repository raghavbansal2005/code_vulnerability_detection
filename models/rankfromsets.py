import torch
import torch.nn as nn
import numpy as np
import psutil

# inner product set-based model using embedding bag and dot product
class InnerProduct(nn.Module):
    def __init__(self, n_labels, n_attributes, emb_size, sparse, mode, all_labels=True):
        super().__init__()
        self.emb_size = emb_size
        self.n_labels = n_labels
        self.label_embs = nn.Embedding(n_labels, emb_size, sparse=sparse)
        self.label_bias = nn.Embedding(n_labels, 1, sparse=sparse)
        self.attribute_emb_bag = nn.EmbeddingBag(n_attributes, emb_size, mode=mode, sparse=sparse)
        self.attribute_bias_bag = nn.EmbeddingBag(n_attributes, 1, mode=mode, sparse=sparse)
        self.all_labels = all_labels
        # self.reset_parameters()

    def reset_parameters(self):
        for module in [self.label_embs, self.attribute_emb_bag]:
            scale = 0.07
            nn.init.uniform_(module.weight, -scale, scale)
        for module in [self.label_bias, self.attribute_bias_bag]:
            nn.init.zeros_(module.weight)

    def collate_fn(self, examples):
        words = []
        for idx, example in enumerate(examples):
            words.append(list(set(example["tokenized_code"])))

        labels = torch.tensor(np.stack([x["vulnerability_id"] for x in examples], axis=0), dtype=torch.long)

        labels_arr = torch.zeros((len(examples), 256), dtype=torch.long)
        for i in range(len(labels)):
            labels_arr[i][labels[i]] = 1

        num_words = [len(x) for x in words]
        num_words.insert(0, 0)
        num_words.pop(-1)
        offsets = torch.tensor(np.cumsum(num_words), dtype=torch.long)

        words = np.concatenate(words, axis=0)
        words = torch.tensor(words, dtype=torch.long)

        batch = {"word_attributes": words, "attribute_offsets": offsets, "labels": labels_arr, "metrics_labels": labels.squeeze()}
        # "publications":torch.tensor(np.ones(len(examples)), dtype=torch.long)
        return batch

    def collate_fn_binary(self, examples):
        words = []
        for idx, example in enumerate(examples):
            words.append(list(set(example["tokenized_code"])))

        labels = torch.tensor(np.stack([x["binary_label"] for x in examples], axis=0), dtype=torch.long)

        labels_arr = torch.zeros((len(examples), 2), dtype=torch.long)
        for i in range(len(labels)):
            labels_arr[i][labels[i]] = 1

        num_words = [len(x) for x in words]
        num_words.insert(0, 0)
        num_words.pop(-1)
        offsets = torch.tensor(np.cumsum(num_words), dtype=torch.long)

        words = np.concatenate(words, axis=0)
        words = torch.tensor(words, dtype=torch.long)

        batch = {"word_attributes": words, "attribute_offsets": offsets, "labels": labels_arr, "metrics_labels": labels.squeeze()}
        # "publications":torch.tensor(np.ones(len(examples)), dtype=torch.long)
        return batch


    def forward(self, word_attributes, attribute_offsets, labels=None, return_intermediate=False):

        if self.all_labels:
            label_emb = self.label_embs.weight
            label_bias = self.label_bias.weight
        else:
            label_emb = self.label_embs(labels)
            label_bias = self.label_bias(labels)


        # print(label_emb.shape)

        attr_emb = self.attribute_emb_bag(word_attributes, attribute_offsets)
        attr_bias = self.attribute_bias_bag(word_attributes, attribute_offsets)
        
        # print(attr_emb.shape)

        # (batch_size, emb_size) x (emb_size, num_labels) -> (batch_size, num_labels)
        inner_prod = attr_emb @ label_emb.t()
        logits = inner_prod + label_bias.t()  # broadcasting across publication dimension
        logits += attr_bias  # broadcast across article dimension

        if return_intermediate:
            return logits, label_emb, attr_emb
        else:
            return logits

    # def forward(self, publications, word_attributes, attribute_offsets, **kwargs):
    #     publication_emb = self.label_embs(publications)
    #     attribute_emb = self.attribute_emb_bag(word_attributes, attribute_offsets)
    #     article_and_attr_emb = attribute_emb
    #     attr_bias = self.attribute_bias_bag(word_attributes, attribute_offsets)
    #     publication_bias = self.label_bias(publications)
    #     # for every publication, only compute inner product with corresponding minibatch element
    #     # (batch_size, 1, emb_size) x (batch_size, emb_size, 1) -> (batch_size, 1)
    #     # logits = torch.bmm(publication_emb.view(-1, 1, self.emb_size),
    #     #                    (article_and_attr_emb).view(-1, self.emb_size, 1)).squeeze()
    #     inner_prod = (publication_emb * article_and_attr_emb).sum(-1)
    #     logits = inner_prod + attr_bias.squeeze() + publication_bias.squeeze()
    #     return logits