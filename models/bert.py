from transformers import BertConfig, BertModel, BertForPreTraining, BertForSequenceClassification
from models.utils import NeuralNetwork
import torch
import torch.nn as nn
from torch.nn.utils.rnn import pad_sequence
import numpy as np
from util import _truncate_seq_pair


class BERTbase(nn.Module):
    def __init__(self, config_file, tokenizer):
        super().__init__()
        self.tokenizer = tokenizer
        self.vocab = tokenizer.get_vocab()
        self.special_tokens = [self.vocab["[SEP]"], self.vocab["[CLS]"], self.vocab["[MASK]"], self.vocab["[PAD]"]]
        if config_file:
            self.config = BertConfig.from_json_file(config_file)
        else:
            self.config = BertConfig()

    def tensorize_batch(self, inputs):
        length_of_first = inputs[0].size(0)
        are_tensors_same_length = all(x.size(0) == length_of_first for x in inputs)
        if are_tensors_same_length:
            return torch.stack(inputs, dim=0)
        else:
            return pad_sequence(inputs, batch_first=True, padding_value=self.vocab["[PAD]"])


class BERTforClassification(BERTbase):
    def __init__(self, pretrained_file, n_labels, tokenizer):
        super().__init__(tokenizer)
        if pretrained_file:
            self.bert_model = BertForSequenceClassification.from_pretrained(pretrained_file, config=self.config)
        else:
            self.bert_model = BertForSequenceClassification.from_pretrained("bert-base-uncased")
        self.bert_model.classifier = nn.Linear(self.bert_model.config.hidden_size, n_labels)
        self.bert_model.num_labels = n_labels

    def forward(self, input_ids, attention_mask):
        logits = self.bert_model(input_ids=input_ids, attention_mask=attention_mask, return_dict=False)
        return logits[0]

    def create_features(self, tokens):
        input_id = [self.vocab["[CLS]"]] + tokens[:510] + [self.vocab["[SEP]"]]
        attention_mask = [1] * len(input_id)

        input_id.extend([0] * (512 - len(input_id)))
        attention_mask.extend([0] * (512 - len(attention_mask)))

        return input_id, attention_mask

    def collate_fn(self, examples):
        input_ids = []
        attention_mask = []
        labels = torch.tensor(np.stack([x[1] for x in examples], axis=0), dtype=torch.long)

        for _, example in enumerate(examples):
            input_id, mask = self.create_features(example[0]["tokenized_code"])
            input_ids.append(input_id)
            attention_mask.append(mask)

        input_ids = torch.tensor(input_ids, dtype=torch.long)
        attention_mask = torch.tensor(attention_mask, dtype=torch.long)

        return {
            "input_ids": input_ids,
            "attention_mask": attention_mask,
            "labels": labels,
        }